\documentclass[12pt,a4]{article}

\usepackage{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{url}

\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\ra}{\rightarrow}
\newcommand{\eps}{\varepsilon}
\newcommand{\cond}{\ensuremath{\text{cond}}}



\title{Inversion of the Laplace transform}
\author{Lasse Lybeck\\013748498}


\begin{document}

\maketitle

\section{Introduction}

Let $f:[0,\infty)\rightarrow \R$. The Laplace transform $F$ of $f$ is defined by
\begin{equation}\label{laplace}
 F(s) = \int_0^\infty e^{-st}f(t)dt,\quad s\in\C ,
\end{equation}
provided that the integral converges.

The Laplace transform has many applications in physical sciences and is therefore widely used and studied. One example of the usage of the transform is in differential equations, which may in some cases easily be solved using the Laplace transform. These kinds of equations appear in many problems, including electric circuit theory and harmonic vibrations of beams. \cite{laplaceweb} \cite{transformsbook}

The direct problem is to determine $F$ for a given function $f$ according to (\ref{laplace}). The inverse problem is: {\em given a Laplace transform $F$, find the corresponding function $f$.} In this study we will be looking at the inverse problem from the computational point of view. We will notice that the inverse problem is ill-posed and not at all trivial.




\section{Materials and Methods}\label{sec:methods}

\subsection{Theoretical basis}

In this study we will solve the inverse problem with the \emph{truncated singular value decomposition} method. In the future we will refer to the singular value decomposition as SVD. Let us first revise the theory for the SVD and the pseudoinverse.

\subsubsection{Singular value decomposition and the pseudoinverse}
\label{sec:svd}

The truncated SVD method is based on the fact that every matrix $A \in \R^{k \times n}$ can be decomposed into the product of three matrices
\begin{equation}
A = U D V^T,
\end{equation}
where $U$ and $V$ are orthogonal matrices and $D$ is a diagonal matrix. The diagonal elements $d_{i,i}$, $i = 1, \ldots , \min \left\{ k,n \right\}$ of $D$ are called the \emph{singular values of $D$}.

The \emph{pseudoinverse} of $A$ (denoted as $A^+$) can be calculated via the SVD of $A$. We define the pseudoinverse of the diagonal matrix $D \in \R^{k \times n}$ as the diagonal matrix $D^+ \in \R^{n \times k}$ where the diagonal elements have the values
\begin{equation}
D^+_{i,i} =
\begin{cases}
    1 / d_{i,i} & \text{if }d_{i,i} \neq 0 \\
    0           & \text{otherwise}.
\end{cases}
\end{equation}
Now we can define the pseudoinverse of $A$ as 
\begin{equation}\label{pseudo}
A^+ = V D^+ U^T .
\end{equation}

It can be shown, that if $A$ is an invertible matrix, then $A^+ = A^{-1}$. This is however often not the case.

In the case of linear systems $Af = m$ we get the least squares solution easily with the pseudo inverse of $A$. As it is shown in \cite{samu}, theorem 4.1, the least squares solution to a system $Af = m$, where $A \in \R^{k \times n}$, is given by $A^+ m$. Notice that here $A$ need not be invertible, as it may not even be a square matrix.

\subsubsection{Truncated singular value decomposition}

Although we now know that linear systems of the type $Af = m$ can easily be solved with the pseudoinverse of the coefficient matrix $A$, this will not always be the proper way of solving inverse problems of the same type. Consider the following case.

Let $m = A f + \eps$, where $\eps$ is some small random noise. It would seem logical that this kind of a problem could be solved approximately with the same method as the problems described earlier with no noise present, as $Af + \eps \approx Af$. This will however not work, as we will come to see later. The reason to this comes from the so called \emph{condition number} of the coefficient matrix $A$.

The condition number of a matrix $A \in \R^{k \times n}$ is defined as the ratio of the largest and smallest singular of $A$. In the case $A$ has an SVD as described in section \ref{sec:svd}, the condition number is defined as $\cond(A) = d_1 / d_{\min(k,n)}$.

In the case of ill-posed inverse problems the condition numbers are large and grow as the size of the matrix grows. This is the reason why naive inversion fails for these problems. As the condition number is large, the least singular values are correspondingly very small. This means that for the pseudoinverse $A^+$ the singular values are very large. As we try to calculate the least squares solution with the naive inversion the right side of the equation becomes $A^+ (Af + \eps)$, where we multiply $A^+$ with the noise $\eps$. This leads to a large error, even thou the noise is quite small, as the inverse singular values are often very much larger. This is the reason why naive inversion fails for ill-posed inverse problems.

The truncated SVD is a method that tries to solve this problem by "discarding" the singular values that are regarded as too small. For a \emph{regularization parameter} $\alpha > 0$, the truncated SVD for a matrix $A \in \R^{k \times n}$ in defined in the means of the SVD $A = U D V^T$ as $A_{\alpha} = U D_{\alpha} V^T$, where
\begin{equation}
(D_{\alpha})_{i,i} =
\begin{cases}
D_{i,i}, & \text{if } D_{i,i} \geq \alpha \\
0,       & \text{otherwise}.
\end{cases}
\end{equation}

Now the least singular value of $A_{\alpha}$ is no smaller that $\alpha$, and the problem with the naive inversion is solved, as long as the regularization parameter can be chosen well enough. We now have a reconstruction function
\begin{equation}
T_{\alpha}(m) = A_{\alpha}^+ m = V D_{\alpha} U^T m.
\end{equation}
This reconstruction model will be used in this study to solve the inverse Laplace transform. For further reading on truncated SVD, see \cite{samu}, chapter 4.2.



\subsection{The matrix model}\label{sec:matrixmodel}

Assume we know the values of $F$ at these real-valued points:
$$
 0<s_1<s_2<\ldots <s_n<\infty.
$$ 
Then we may approximate the integral in (\ref{laplace}) for example with the trapezoidal rule as
\begin{equation} \label{laptrap}
\begin{split}
 \int_0^\infty e^{-st}f(t)dt\, \approx\, \frac{t_k}{k} & \left( \frac{1}{2}e^{-st_1}f(t_1)+e^{-st_2}f(t_2)+e^{-st_3}f(t_3)+\ldots\right.\\   &\ \ \left. +e^{-st_{k-1}}f(t_{k-1})+\frac{1}{2}e^{-st_k}f(t_k)\right) ,
\end{split}
\end{equation}
where vector $t=[t_1\ t_2\ \ldots\ t_k]^T\in\R^k$, $0\leq t_1<t_2<\ldots <t_k$, contains the points at which the unknown function $f$ will be evaluated. By denoting $f_\ell=f(t_\ell), \ \ell=1,\ldots ,k$, and $m_j=F(s_j),\ j=1,\ldots ,n$, and using \eqref{laptrap}, we get a linear model of the form $m=Af+\epsilon$ with
\begin{equation}\label{LaplaceA} 
A = \frac{t_k}{k}\begin{bmatrix} \frac{1}{2}e^{-s_1t_1} & e^{-s_1t_2} & e^{-s_1t_3} & \ldots & e^{-s_1t_{k-1}} & \frac{1}{2}e^{-s_1t_k} \\
                       \frac{1}{2}e^{-s_2t_1} & e^{-s_2t_2} & e^{-s_2t_3} & \ldots & e^{-s_2t_{k-1}} & \frac{1}{2}e^{-s_2t_k} \\
                       \vdots & & & & & \vdots \\
                       \frac{1}{2}e^{-s_nt_1} & e^{-s_nt_2} & e^{-s_nt_3} & \ldots & e^{-s_nt_{k-1}} & \frac{1}{2}e^{-s_nt_k} \end{bmatrix}.
\end{equation}


\subsection{The inversion method}
\label{sec:invmethod}

As the materials for this study we have created MATLAB code for calculating the inverse Laplace transform with the truncated SVD method. The starting point for out experiments is as follows.

The measurements were done with the function $f: \left[ 0, \infty \right[ \ra \R$,
\begin{equation}\label{eq:f}
f(t) = 
\begin{cases}
1, & \text{for } 0 \leq t \leq 1 \\
0, & \text{otherwise}.
\end{cases}
\end{equation}

The matrix $A$ and vectors $s$ and $t$ are defined as explained in section \ref{sec:matrixmodel}. The values of $s_i$ of the vector $s \in \R^n$ and $t_j$ of the vector $t \in \R^k$ were chosen evenly spaced from the intervals $\left] 0, 100 \right[$ and $\left[ 0,3 \right]$, respectively. The values of $n$ and $k$ were varied in the experiments to determine their effect on the condition of the coefficient matrix. In the recorded reconstructions the values were $n = 300$ and $k = 1000$.

With the specified function $f$ the Laplace transform can be calculated as
\begin{equation}
F(s) = \int_0^{\infty} e^{-st} f(t) dt
     = \int_0^1 e^{-st} dt
     = \frac{1 - e^{-s}}{s},
\end{equation}
and thus the values of the Laplace transform can be easily computed without using numerical integration methods. This adds both speed and precision to the calculations.

We then created the measurement points $m = [m_1, m_2, \ldots, m_n]^T + \eps$, where $m_i = F(s_i)$, $F$ is defined as in \eqref{laplace} for the function $f$ and the elements $\eps_k$ of the vector $\eps$ are samples from a normal distribution to simulate random noise. The noise used in the reconstruction had the standard deviation of $1 \cdot 10^{-5}$.

The reconstruction $T_\alpha(m)$ from the measurement data $m$ of the function $f$ in the interval $\left[0,3\right]$ could then be calculated with the truncated SVD method with $\alpha$ as the regularization parameter. The results were then recorded with different choices of $\alpha$.



\section{Results}\label{sec:results}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=.5]{img/laplace_data.png}
\end{center}
\caption{Left: Laplace transform of $f$. Right: An example of synthetic data used in the experiments.}
\label{fig:laplace}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=.4]{img/singular.png}
\includegraphics[scale=.4]{img/cond.png}
\end{center}
\caption{Left: Singular values of a Laplace transform matrix. Right: Condition numbers of $k \times k$ Laplace transform matrices}
\label{fig:singular}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}[t]{|c|c|c|}
\hline
Regularization parameter $\alpha$ & Singular values & Relative error
\\ \hline
$1 \cdot 10^{-1}$  &  1    & 63.9              \% \\ \hline
$1 \cdot 10^{-2}$  &  5    & 36.2              \% \\ \hline
$1 \cdot 10^{-3}$  &  8    & 27.4              \% \\ \hline
$1 \cdot 10^{-4}$  &  10   & 25.2              \% \\ \hline
$1 \cdot 10^{-5}$  &  13   & 21.8              \% \\ \hline
$1 \cdot 10^{-6}$  &  15   & 25.9              \% \\ \hline
$1 \cdot 10^{-7}$  &  17   & 284               \% \\ \hline
$1 \cdot 10^{-8}$  &  20   & 3904              \% \\ \hline
$1 \cdot 10^{-9}$  &  22   & 56049             \% \\ \hline
$1 \cdot 10^{-10}$ &  24   & 308059            \% \\ \hline
$0$                &  300  & $7.99 \cdot 10^8$ \% \\ \hline
\end{tabular}
\end{center}
\caption{Reconstruction error with different values of the regularization parameter $\alpha$.}
\label{tab:recs}
\end{table}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=.5]{img/reconstruction.png}
\includegraphics[scale=.4]{img/reconstruction_large.png}
\includegraphics[scale=.4]{img/reconstruction_small.png}
\end{center}
\caption{Top: The inverse Laplace transform reconstruction of the function with the best regularization parameter. Bottom: Reconstruction with a regularization parameter that is (left) big (right) small.}
\label{fig:reconstruction}
\end{figure}


The Laplace transform of the function $f$ defined in \eqref{eq:f} is shown in figure \ref{fig:laplace}. An example of the synthetic measurement data used in the inversion of the Laplace transform can be seen in the right hand side plot in the same figure.

In the left hand side plot in figure \ref{fig:singular} the singular values are shown for a certain Laplace transform matrix $A$. The matrix is constructed with the vectors $t \in \R^k$ and $s \in \R^n$, with the values $k = 1000$ and $n = 300$. The condition number for the matrix is $\cond(A) \approx 8.6407 \cdot 10^{17}$. In the right hand side plot is shown the condition number for a collection of square Laplace transform matrices. The number of rows and columns of the square matrices in the plot ranges from $1$ to $500$.

In table \ref{tab:recs} is shown the relative error of the reconstructions of the function $f$ with different regularization parameters. The parameters for the data and the method of the reconstruction are as described in section \ref{sec:invmethod}. The relative error is calculated with the formula
\begin{equation}
\delta_{rel} = \frac{\left\| f - T_{\alpha}(m) \right\|_2}{\left\| f \right\|_2} \cdot 100 \%,
\end{equation}
where $f = (f_i)_{i=1}^n$ is the vector of the correct values of the function and $T_{\alpha}(m)$ is the reconstruction calculated from the noisy data $m$ with the regularization parameter $\alpha$.

The reconstructions corresponding to the data in the table are shown for some values of the regularization parameter in figure \ref{fig:reconstruction}. In the top most image is shown the reconstruction with the smallest relative error, which was achieved with the value $\alpha = 1 \cdot 10^{-5}$ of the regularization parameter. The two lower most images show the reconstruction with the values $\alpha = 1 \cdot 10^{-2}$ and $\alpha = 1 \cdot 10^{-7}$ of the regularization parameter, in the left and right images, respectively.


\section{Discussion}

As it can be seen from the left hand side image in figure \ref{fig:singular}, the singular values of the coefficient matrix are decreasing very rapidly (notice the logarithmic scale on the y-axis). This results in high condition numbers and therefore in systems with high sensibility for noise. As it can further be seen from right hand side image, the condition number greatly increases as the size of the matrix increases (again, notice the logarithmic scale). This clearly points to the fact that the inverse Laplace transform is an ill-posed inverse problem.

From table \ref{tab:recs} we can also see that the naive reconstruction (with the regularization parameter $\alpha = 0$) fails spectacularly. This is also an indication to the fact that we are dealing with an ill-posed inverse problem.

As it can be seen from the relative errors of the reconstructions with different values of regularization parameter, the error decreases at first, but then again increases after hitting a lowest error at $\alpha = 1 \cdot 10^{-5}$. From figure \ref{fig:reconstruction} we can see what happens with different values of the regularization parameter.

As the parameter is big, as $\alpha = 1 \cdot 10^{-2}$ in the bottom left image in figure \ref{fig:reconstruction}, the reconstruction has not quite achieved the shape of the function, but it is rather much like a smoothed version of the function. This is due to the fact, that with a big value of $\alpha$ many singular values are discarded from the matrix, and information about the measurements is lost.

With a small parameter, as in the bottom right image of figure \ref{fig:reconstruction}, where $\alpha = 1 \cdot 10^{-7}$, the small singular values of the coefficient matrix amplify the noise from the measurements. Therefore the reconstruction gets more and more oscillatory as the parameter gets smaller, and the relative error of the reconstruction increases rapidly.



\newpage
\begin{thebibliography}{9}

\bibitem{laplaceweb}
Adhikari, S. (2008). Laplace transforms and its applications. Available: \url{http://sces.phys.utk.edu/~moreo/mm08/sarina.pdf}. Last accessed 12th Mar 2013.

\bibitem{samu}
Mueller, Jennifer L., ; Siltanen, Samuli (2012)
\emph{Linear and nonlinear inverse problems with practical applications} : SIAM

\bibitem{transformsbook}
Poularikas A. (2012). Chapter 5, \emph{Laplace transforms} in \emph{Transforms and Applications Handbook, Third Edition} (Electrical Engineering Handbook) : CRC Press


\end{thebibliography}

\end{document}



